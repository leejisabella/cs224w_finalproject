{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02bb5713",
      "metadata": {
        "state": {},
        "id": "02bb5713"
      },
      "source": [
        "Environment and Import Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3fe805ec",
      "metadata": {
        "id": "3fe805ec"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio -q\n",
        "!pip install torch-geometric -q\n",
        "!pip install dgl -q  # generic DGL (CPU/GPU autodetect)\n",
        "!pip install torchmetrics==1.4.0.post0 scikit-learn pandas numpy tqdm geopy haversine -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "901c809e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901c809e",
        "outputId": "01c15fe5-72ec-4a2b-e373-233141e3f83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, json, math, random, gc, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.utils import to_undirected, coalesce\n",
        "from torch_geometric.nn import HGTConv, SAGEConv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from haversine import haversine\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED);\n",
        "if DEVICE.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8008a98",
      "metadata": {
        "id": "e8008a98"
      },
      "source": [
        "Data Loading and Graph Construction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"filter_all_t.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(type(data))\n",
        "print(list(data.keys())[:10] if isinstance(data, dict) else data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWGABdqNS82v",
        "outputId": "906e53f5-26d1-4a8a-f459-26e436b0106a"
      },
      "id": "FWGABdqNS82v",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "['train', 'val', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"filter_all_t.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Loop through each dataset and save separately\n",
        "for split_name, records in data.items():\n",
        "    df = pd.DataFrame(records)\n",
        "    csv_name = f\"{split_name}.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    print(f\"✅ Saved {csv_name} with {len(df)} rows.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY42lM0BPZT6",
        "outputId": "9ad2b3f0-9508-4ed1-a72b-65b3dd83e1a3"
      },
      "id": "DY42lM0BPZT6",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved train.csv with 87013 rows.\n",
            "✅ Saved val.csv with 10860 rows.\n",
            "✅ Saved test.csv with 11015 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79137a93",
      "metadata": {
        "id": "79137a93"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/data\"  # <- change this to your path in Drive or Colab\n",
        "REVIEWS_CSV = os.path.join(DATA_DIR, \"reviews.csv\")\n",
        "REVIEWS_JSONL = os.path.join(DATA_DIR, \"reviews.jsonl\")\n",
        "RESTAURANTS_JSON = os.path.join(DATA_DIR, \"restaurants.json\")   # nested form like screenshot\n",
        "MIN_USER_INTERACTIONS = 2\n",
        "MIN_ITEM_INTERACTIONS = 2\n",
        "\n",
        "def _safe_float(x, default=np.nan):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def load_google_restaurants() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with columns:\n",
        "    user_id, item_id, rating, ts (unix int),\n",
        "    user_lat, user_lon (optional), item_lat, item_lon,\n",
        "    price (int 1-4 or str like '$$'), categories (list[str])\n",
        "    \"\"\"\n",
        "    if os.path.exists(REVIEWS_CSV):\n",
        "        df = pd.read_csv(REVIEWS_CSV)\n",
        "    elif os.path.exists(REVIEWS_JSONL):\n",
        "        df = pd.read_json(REVIEWS_JSONL, lines=True)\n",
        "    elif os.path.exists(RESTAURANTS_JSON):\n",
        "        # Expect either one JSON array or JSONL with one restaurant per line\n",
        "        rows = []\n",
        "        def normalize_price(p):\n",
        "            if isinstance(p, str) and p.count(\"$\")>0:\n",
        "                return len(p)\n",
        "            return int(p) if pd.notna(p) else np.nan\n",
        "        with open(RESTAURANTS_JSON,'r') as f:\n",
        "            txt = f.read().strip()\n",
        "            if txt.startswith('['):\n",
        "                items = json.loads(txt)\n",
        "            else:\n",
        "                items = [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
        "        for place in tqdm(items, desc=\"Parsing restaurants.json\"):\n",
        "            pid = place.get(\"gmap_url\") or place.get(\"place_id\") or place.get(\"name\")\n",
        "            ilat = _safe_float(place.get(\"Latitude\"))\n",
        "            ilon = _safe_float(place.get(\"Longitude\"))\n",
        "            price = normalize_price(place.get(\"price\", np.nan))\n",
        "            cats = place.get(\"category\", [])\n",
        "            if isinstance(cats,str): cats=[cats]\n",
        "            revs = place.get(\"Reviews\") or place.get(\"reviews\") or []\n",
        "            for r in revs:\n",
        "                rows.append({\n",
        "                    \"user_id\": str(r.get(\"user_id\") or r.get(\"userId\") or r.get(\"uid\")),\n",
        "                    \"item_id\": str(pid),\n",
        "                    \"rating\": _safe_float(r.get(\"Rating\") or r.get(\"rating\") or 0.0),\n",
        "                    \"ts\": int(time.time()) if r.get(\"time\") is None else int(time.time()) if isinstance(r.get(\"time\"), str) else int(r.get(\"time\")),\n",
        "                    \"user_lat\": np.nan, \"user_lon\": np.nan,\n",
        "                    \"item_lat\": ilat, \"item_lon\": ilon,\n",
        "                    \"price\": price, \"categories\": cats\n",
        "                })\n",
        "        df = pd.DataFrame(rows)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Place your dataset at /content/data. Supported files: reviews.csv, reviews.jsonl, or restaurants.json\")\n",
        "\n",
        "    # Clean/standardize\n",
        "    for col in [\"user_id\",\"item_id\"]:\n",
        "        df[col] = df[col].astype(str)\n",
        "    if \"rating\" not in df: df[\"rating\"] = 5.0\n",
        "    if \"ts\" not in df: df[\"ts\"] = int(time.time())\n",
        "    for c in [\"item_lat\",\"item_lon\",\"user_lat\",\"user_lon\"]:\n",
        "        if c not in df: df[c] = np.nan\n",
        "    if \"price\" not in df: df[\"price\"] = np.nan\n",
        "    if \"categories\" not in df: df[\"categories\"] = [[] for _ in range(len(df))]\n",
        "    return df\n",
        "\n",
        "df = load_google_restaurants()\n",
        "print(df.head(), \"\\n\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce7adfe",
      "metadata": {
        "id": "fce7adfe"
      },
      "outputs": [],
      "source": [
        "# Filter to users/items with minimum interactions\n",
        "def filter_min_interactions(df, umin=MIN_USER_INTERACTIONS, imin=MIN_ITEM_INTERACTIONS):\n",
        "    grouped_u = df.groupby(\"user_id\").size()\n",
        "    keep_users = set(grouped_u[grouped_u>=umin].index)\n",
        "    grouped_i = df.groupby(\"item_id\").size()\n",
        "    keep_items = set(grouped_i[grouped_i>=imin].index)\n",
        "    out = df[df.user_id.isin(keep_users) & df.item_id.isin(keep_items)].copy()\n",
        "    return out\n",
        "\n",
        "df = filter_min_interactions(df)\n",
        "print(\"After filtering:\", df.shape)\n",
        "\n",
        "# Encode ids -> indices\n",
        "u_enc = LabelEncoder().fit(df[\"user_id\"])\n",
        "i_enc = LabelEncoder().fit(df[\"item_id\"])\n",
        "df[\"u\"] = u_enc.transform(df[\"user_id\"])\n",
        "df[\"i\"] = i_enc.transform(df[\"item_id\"])\n",
        "\n",
        "num_users = df[\"u\"].max()+1\n",
        "num_items = df[\"i\"].max()+1\n",
        "print(f\"num_users={num_users}, num_items={num_items}\")\n",
        "\n",
        "# Train/val/test split by user (leave-one-out style)\n",
        "df = df.sort_values(\"ts\")\n",
        "def split_by_user(group):\n",
        "    if len(group) < 3:  # small fallback\n",
        "        test = group.iloc[[-1]]\n",
        "        val = group.iloc[[-2]] if len(group)>=2 else group.iloc[[-1]]\n",
        "        train = group.drop(test.index).drop(val.index, errors='ignore')\n",
        "    else:\n",
        "        test = group.iloc[[-1]]\n",
        "        val = group.iloc[[-2]]\n",
        "        train = group.iloc[:-2]\n",
        "    return train, val, test\n",
        "\n",
        "train_rows, val_rows, test_rows = [], [], []\n",
        "for uid, g in df.groupby(\"u\"):\n",
        "    tr, va, te = split_by_user(g)\n",
        "    train_rows.append(tr); val_rows.append(va); test_rows.append(te)\n",
        "train_df = pd.concat(train_rows); val_df = pd.concat(val_rows); test_df = pd.concat(test_rows)\n",
        "\n",
        "# Build hetero graph (user<->item); also build item<->item edges for PinSage\n",
        "data = HeteroData()\n",
        "data[\"user\"].num_nodes = num_users\n",
        "data[\"item\"].num_nodes = num_items\n",
        "\n",
        "# user-item edges (train only for supervision)\n",
        "ui_src = torch.tensor(train_df[\"u\"].values, dtype=torch.long)\n",
        "ui_dst = torch.tensor(train_df[\"i\"].values, dtype=torch.long)\n",
        "edge_index = torch.stack([ui_src, ui_dst], dim=0)\n",
        "data[\"user\",\"rates\",\"item\"].edge_index = edge_index\n",
        "data[\"item\",\"rev_by\",\"user\"].edge_index = edge_index.flip(0)\n",
        "\n",
        "# item coordinates + metadata (optional features)\n",
        "item_lat = torch.full((num_items,), float('nan'))\n",
        "item_lon = torch.full((num_items,), float('nan'))\n",
        "price = torch.zeros(num_items)\n",
        "for i, sub in df.groupby(\"i\").head(1).groupby(\"i\"):\n",
        "    item_lat[i] = float(sub[\"item_lat\"].iloc[0]) if pd.notna(sub[\"item_lat\"].iloc[0]) else float('nan')\n",
        "    item_lon[i] = float(sub[\"item_lon\"].iloc[0]) if pd.notna(sub[\"item_lon\"].iloc[0]) else float('nan')\n",
        "    pr = sub[\"price\"].iloc[0]\n",
        "    price[i] = float(pr) if pd.notna(pr) else 0.\n",
        "data[\"item\"].x = torch.stack([\n",
        "    torch.nan_to_num(item_lat, nan=0.0),\n",
        "    torch.nan_to_num(item_lon, nan=0.0),\n",
        "    price\n",
        "], dim=1)\n",
        "\n",
        "# Quick item-item edges via co-review + geo proximity\n",
        "co_counts = {}\n",
        "for u, grp in train_df.groupby(\"u\"):\n",
        "    items = grp[\"i\"].tolist()\n",
        "    for a in items:\n",
        "        for b in items:\n",
        "            if a>=b: continue\n",
        "            co_counts[(a,b)] = co_counts.get((a,b),0)+1\n",
        "pairs = [(a,b,c) for (a,b),c in co_counts.items() if c>=2]\n",
        "ii_src = [a for a,b,_ in pairs]; ii_dst = [b for a,b,_ in pairs]\n",
        "\n",
        "# geo proximity (<= 2km)\n",
        "for a in range(num_items):\n",
        "    if math.isnan(item_lat[a]) or math.isnan(item_lon[a]):\n",
        "        continue\n",
        "    for b in range(a+1, min(num_items, a+300)):  # local window to keep edges sparse\n",
        "        if math.isnan(item_lat[b]) or math.isnan(item_lon[b]):\n",
        "            continue\n",
        "        dkm = haversine((float(item_lat[a]), float(item_lon[a])), (float(item_lat[b]), float(item_lon[b])))\n",
        "        if dkm <= 2.0:\n",
        "            ii_src.append(a); ii_dst.append(b)\n",
        "\n",
        "ii_edge = torch.tensor([ii_src, ii_dst], dtype=torch.long)\n",
        "ii_edge = to_undirected(ii_edge)\n",
        "data[\"item\",\"similar\",\"item\"].edge_index = coalesce(ii_edge, num_nodes=num_items)\n",
        "\n",
        "data = data.to(DEVICE)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a8a45a",
      "metadata": {
        "id": "74a8a45a"
      },
      "source": [
        "Sampling, Metrics, and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76b218f",
      "metadata": {
        "id": "b76b218f"
      },
      "outputs": [],
      "source": [
        "def bpr_triplet_sampler(train_df: pd.DataFrame, num_items: int,\n",
        "                        radius_km: Optional[float]=None,\n",
        "                        item_latlon: Optional[List[Tuple[float,float]]]=None,\n",
        "                        user_pos_map: Optional[Dict[int,set]]=None,\n",
        "                        user_home_latlon: Optional[Dict[int,Tuple[float,float]]]=None,\n",
        "                        batch_size: int = 2048):\n",
        "    \"\"\"\n",
        "    Yields batches of (u, pos_i, neg_j) for BPR.\n",
        "    If radius_km is set, negative items are sampled within radius of user's home (or of pos item if home unknown).\n",
        "    \"\"\"\n",
        "    if user_pos_map is None:\n",
        "        user_pos_map = {u:set(g[\"i\"].values.tolist()) for u,g in train_df.groupby(\"u\")}\n",
        "    users = list(user_pos_map.keys())\n",
        "    while True:\n",
        "        uu, ii, jj = [], [], []\n",
        "        for _ in range(batch_size):\n",
        "            u = random.choice(users)\n",
        "            pos_i = random.choice(list(user_pos_map[u]))\n",
        "            # radius-aware negative sampling\n",
        "            if radius_km is not None and item_latlon is not None:\n",
        "                center = None\n",
        "                if user_home_latlon and u in user_home_latlon:\n",
        "                    center = user_home_latlon[u]\n",
        "                else:\n",
        "                    lat_i, lon_i = item_latlon[pos_i]\n",
        "                    if not math.isnan(lat_i) and not math.isnan(lon_i):\n",
        "                        center = (lat_i, lon_i)\n",
        "                candidates = None\n",
        "                if center:\n",
        "                    latc, lonc = center\n",
        "                    idxs = list(range(num_items))\n",
        "                    candidates = [k for k in idxs if k not in user_pos_map[u]]\n",
        "                    random.shuffle(candidates)\n",
        "                    # filter by distance lazily\n",
        "                    neg_j = None\n",
        "                    for k in candidates:\n",
        "                        latk, lonk = item_latlon[k]\n",
        "                        if math.isnan(latk) or math.isnan(lonk):\n",
        "                            continue\n",
        "                        if haversine((latc,lonc),(latk,lonk)) <= radius_km:\n",
        "                            neg_j = k; break\n",
        "                    if neg_j is None:\n",
        "                        # fallback random\n",
        "                        while True:\n",
        "                            k = random.randrange(num_items)\n",
        "                            if k not in user_pos_map[u]:\n",
        "                                neg_j = k; break\n",
        "                else:\n",
        "                    # fallback random\n",
        "                    while True:\n",
        "                        k = random.randrange(num_items)\n",
        "                        if k not in user_pos_map[u]:\n",
        "                            neg_j = k; break\n",
        "            else:\n",
        "                # uniform negative\n",
        "                while True:\n",
        "                    k = random.randrange(num_items)\n",
        "                    if k not in user_pos_map[u]:\n",
        "                        neg_j = k; break\n",
        "            uu.append(u); ii.append(pos_i); jj.append(neg_j)\n",
        "        yield torch.tensor(uu, device=DEVICE), torch.tensor(ii, device=DEVICE), torch.tensor(jj, device=DEVICE)\n",
        "\n",
        "# Ranking metrics\n",
        "def recall_at_k(ranked_items, ground_truth, k=10):\n",
        "    hits = sum([1 for x in ranked_items[:k] if x in ground_truth])\n",
        "    return hits / float(min(k, len(ground_truth))) if ground_truth else 0.0\n",
        "\n",
        "def ndcg_at_k(ranked_items, ground_truth, k=10):\n",
        "    dcg = 0.0\n",
        "    for idx, it in enumerate(ranked_items[:k], start=1):\n",
        "        if it in ground_truth:\n",
        "            dcg += 1.0 / math.log2(idx+1)\n",
        "    idcg = sum([1.0 / math.log2(i+2) for i in range(min(k, len(ground_truth)))])\n",
        "    return dcg / idcg if idcg>0 else 0.0\n",
        "\n",
        "def geo_discount(distance_km, R=5.0):\n",
        "    return math.exp(-distance_km / R)\n",
        "\n",
        "def geo_ndcg_at_k(ranked_items, ground_truth, user_loc, item_latlon, k=10, R=5.0):\n",
        "    dcg = 0.0\n",
        "    for idx, it in enumerate(ranked_items[:k], start=1):\n",
        "        if it in ground_truth:\n",
        "            if user_loc and not any(np.isnan(user_loc)):\n",
        "                latu, lonu = user_loc\n",
        "                lati, loni = item_latlon[it]\n",
        "                if not math.isnan(lati) and not math.isnan(loni):\n",
        "                    d = haversine((latu,lonu),(lati,loni))\n",
        "                    w = geo_discount(d, R=R)\n",
        "                else:\n",
        "                    w = 1.0\n",
        "            else:\n",
        "                w = 1.0\n",
        "            dcg += w / math.log2(idx+1)\n",
        "    # ideal geo-dcg assume w=1 for positives (upper bound)\n",
        "    idcg = sum([1.0 / math.log2(i+2) for i in range(min(k, len(ground_truth)))])\n",
        "    return dcg / idcg if idcg>0 else 0.0\n",
        "\n",
        "def mrr(ranked_items, ground_truth, k=10):\n",
        "    for idx, it in enumerate(ranked_items[:k], start=1):\n",
        "        if it in ground_truth:\n",
        "            return 1.0 / idx\n",
        "    return 0.0\n",
        "\n",
        "def rmse(preds, trues):\n",
        "    return float(np.sqrt(np.mean((np.array(preds)-np.array(trues))**2)))\n",
        "\n",
        "def mae(preds, trues):\n",
        "    return float(np.mean(np.abs(np.array(preds)-np.array(trues))))\n",
        "\n",
        "# Helper: materialize item latlon for geo bits\n",
        "item_latlon = [(float(v[0]), float(v[1])) for v in data[\"item\"].x[:, :2].tolist()]\n",
        "user_home = {}  # if you have per-user coords, populate {u: (lat,lon)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51a5158",
      "metadata": {
        "id": "e51a5158"
      },
      "source": [
        "Baseline Model: LightGCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "663c9942",
      "metadata": {
        "id": "663c9942"
      },
      "outputs": [],
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, emb_dim=64, num_layers=3, alpha=None):\n",
        "        super().__init__()\n",
        "        self.num_users, self.num_items = num_users, num_items\n",
        "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
        "        self.item_emb = nn.Embedding(num_items, emb_dim)\n",
        "        nn.init.normal_(self.user_emb.weight, std=0.1)\n",
        "        nn.init.normal_(self.item_emb.weight, std=0.1)\n",
        "        self.num_layers = num_layers\n",
        "        self.alpha = alpha if alpha is not None else [1/(num_layers+1)]*(num_layers+1)\n",
        "        # precompute normalized adjacency indices for propagation (bipartite)\n",
        "        edge = data[\"user\",\"rates\",\"item\"].edge_index\n",
        "        u,i = edge[0], edge[1]\n",
        "        deg_u = torch.bincount(u, minlength=num_users).float()\n",
        "        deg_i = torch.bincount(i, minlength=num_items).float()\n",
        "        self.pairs = (u,i,deg_u,deg_i)\n",
        "\n",
        "    def propagate(self, user_x, item_x):\n",
        "        u,i,deg_u,deg_i = self.pairs\n",
        "        all_user = [user_x]; all_item = [item_x]\n",
        "        for _ in range(self.num_layers):\n",
        "            # message passing along normalized bipartite edges\n",
        "            msg_u = torch.zeros_like(user_x)\n",
        "            msg_i = torch.zeros_like(item_x)\n",
        "            # item -> user\n",
        "            msg_u.index_add_(0, u, item_x[i] / torch.sqrt(deg_u[u].unsqueeze(1)*deg_i[i].unsqueeze(1)+1e-8))\n",
        "            # user -> item\n",
        "            msg_i.index_add_(0, i, user_x[u] / torch.sqrt(deg_i[i].unsqueeze(1)*deg_u[u].unsqueeze(1)+1e-8))\n",
        "            user_x, item_x = msg_u, msg_i\n",
        "            all_user.append(user_x); all_item.append(item_x)\n",
        "        # layer-wise averaging\n",
        "        user_out = torch.stack([a*b for a,b in zip(self.alpha, all_user)]).sum(0)\n",
        "        item_out = torch.stack([a*b for a,b in zip(self.alpha, all_item)]).sum(0)\n",
        "        return user_out, item_out\n",
        "\n",
        "    def forward(self):\n",
        "        u0 = self.user_emb.weight\n",
        "        i0 = self.item_emb.weight\n",
        "        return self.propagate(u0, i0)\n",
        "\n",
        "    def score(self, users, items, user_emb=None, item_emb=None):\n",
        "        if user_emb is None or item_emb is None:\n",
        "            user_emb, item_emb = self.forward()\n",
        "        return (user_emb[users] * item_emb[items]).sum(dim=1)\n",
        "\n",
        "def bpr_loss(pos_scores, neg_scores, reg=None, params: List[torch.Tensor]=[]):\n",
        "    loss = -F.logsigmoid(pos_scores - neg_scores).mean()\n",
        "    if reg:\n",
        "        loss = loss + reg*sum(p.norm(2).pow(2) for p in params)/len(params)\n",
        "    return loss\n",
        "\n",
        "def train_lightgcn(epochs=5, emb_dim=64, batch_size=2048, lr=1e-3, reg=1e-4):\n",
        "    model = LightGCN(num_users, num_items, emb_dim=emb_dim).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    sampler = bpr_triplet_sampler(train_df, num_items, batch_size=batch_size)\n",
        "    user_pos = {u:set(g[\"i\"].values.tolist()) for u,g in train_df.groupby(\"u\")}\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for step in range(max(1, len(train_df)//batch_size)):\n",
        "            u,i,j = next(sampler)\n",
        "            user_emb, item_emb = model()\n",
        "            pos = model.score(u,i,user_emb,item_emb)\n",
        "            neg = model.score(u,j,user_emb,item_emb)\n",
        "            loss = bpr_loss(pos,neg,reg, [model.user_emb.weight, model.item_emb.weight])\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total += float(loss)\n",
        "        print(f\"[LightGCN] epoch {ep} loss {total/(step+1):.4f}\")\n",
        "    return model\n",
        "\n",
        "lightgcn_model = train_lightgcn(epochs=5, emb_dim=64, batch_size=2048)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10ae0ee6",
      "metadata": {
        "id": "10ae0ee6"
      },
      "source": [
        "Extension Model 1: LightGCL\n",
        "\n",
        "Elements:\n",
        "1. Contrastive self-supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52be409c",
      "metadata": {
        "id": "52be409c"
      },
      "outputs": [],
      "source": [
        "def build_svd_view(train_df, num_users, num_items, rank=64):\n",
        "    # Sparse user-item matrix\n",
        "    rows = torch.tensor(train_df[\"u\"].values, dtype=torch.long)\n",
        "    cols = torch.tensor(train_df[\"i\"].values, dtype=torch.long)\n",
        "    vals = torch.ones(len(train_df), dtype=torch.float32)\n",
        "    A = torch.sparse_coo_tensor(\n",
        "        indices=torch.stack([rows, cols]), values=vals, size=(num_users, num_items)\n",
        "    ).to_dense()  # for simplicity; if too big, sample or chunk\n",
        "    U, S, Vt = torch.linalg.svd(A, full_matrices=False)\n",
        "    Uk = U[:, :rank] * S[:rank]\n",
        "    Vk = Vt[:rank, :].T * S[:rank]\n",
        "    return Uk.to(DEVICE), Vk.to(DEVICE)\n",
        "\n",
        "def info_nce(z, z_tgt, temperature=0.2):\n",
        "    z = F.normalize(z, dim=1)\n",
        "    z_tgt = F.normalize(z_tgt, dim=1)\n",
        "    logits = z @ z_tgt.T / temperature\n",
        "    labels = torch.arange(z.size(0), device=z.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class LightGCL(nn.Module):\n",
        "    def __init__(self, base: LightGCN, lambda_cl=0.1, svd_rank=64):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.lambda_cl = lambda_cl\n",
        "        self.Uk, self.Vk = build_svd_view(train_df, base.num_users, base.num_items, rank=svd_rank)\n",
        "\n",
        "    def training_step(self, batch, opt, reg=1e-4):\n",
        "        u,i,j = batch\n",
        "        user_emb, item_emb = self.base()\n",
        "        pos = self.base.score(u,i,user_emb,item_emb)\n",
        "        neg = self.base.score(u,j,user_emb,item_emb)\n",
        "        loss_bpr = bpr_loss(pos,neg,reg,[self.base.user_emb.weight, self.base.item_emb.weight])\n",
        "        # contrastive loss: align learned embeddings with SVD embeddings\n",
        "        cl_u = info_nce(user_emb, self.Uk)\n",
        "        cl_i = info_nce(item_emb, self.Vk)\n",
        "        loss = loss_bpr + self.lambda_cl*(cl_u + cl_i)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        return float(loss.item())\n",
        "\n",
        "def train_lightgcl(epochs=5, emb_dim=64, batch_size=2048, lr=1e-3, reg=1e-4, lambda_cl=0.1):\n",
        "    base = LightGCN(num_users, num_items, emb_dim=emb_dim).to(DEVICE)\n",
        "    model = LightGCL(base, lambda_cl=lambda_cl, svd_rank=emb_dim).to(DEVICE)\n",
        "    opt = torch.optim.Adam(base.parameters(), lr=lr)\n",
        "    sampler = bpr_triplet_sampler(train_df, num_items, batch_size=batch_size)\n",
        "    for ep in range(1, epochs+1):\n",
        "        total=0.0\n",
        "        for step in range(max(1, len(train_df)//batch_size)):\n",
        "            u,i,j = next(sampler)\n",
        "            total += model.training_step((u,i,j), opt, reg=reg)\n",
        "        print(f\"[LightGCL] epoch {ep} loss {total/(step+1):.4f}\")\n",
        "    return model\n",
        "\n",
        "lightgcl_model = train_lightgcl(epochs=5, emb_dim=64, lambda_cl=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0d5e11",
      "metadata": {
        "id": "dd0d5e11"
      },
      "source": [
        "Extension Model 2: LightGCL + Geographical Awareness\n",
        "\n",
        "Elements:\n",
        "1. Contrastive self-supervision\n",
        "2. Distance-aware scoring\n",
        "3. Radius-aware negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e3bc18",
      "metadata": {
        "id": "92e3bc18"
      },
      "outputs": [],
      "source": [
        "class LightGCL_Geo(LightGCL):\n",
        "    def __init__(self, base: LightGCN, beta=0.2, R=5.0, lambda_cl=0.1):\n",
        "        super().__init__(base, lambda_cl=lambda_cl, svd_rank=base.user_emb.embedding_dim)\n",
        "        self.beta = beta\n",
        "        self.R = R\n",
        "\n",
        "    def geo_term(self, users, items, user_home, item_latlon):\n",
        "        vals = []\n",
        "        for u,i in zip(users.tolist(), items.tolist()):\n",
        "            uh = user_home.get(u)\n",
        "            if uh is None:\n",
        "                lati,loni = item_latlon[i]\n",
        "                if math.isnan(lati) or math.isnan(loni):\n",
        "                    vals.append(0.0); continue\n",
        "                uh = (lati,loni)  # weak fallback\n",
        "            lati,loni = item_latlon[i]\n",
        "            if math.isnan(lati) or math.isnan(loni):\n",
        "                vals.append(0.0); continue\n",
        "            d = haversine(uh, (lati,loni))\n",
        "            vals.append(math.exp(-d/self.R))\n",
        "        return torch.tensor(vals, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    def training_step(self, batch, opt, reg=1e-4):\n",
        "        u,i,j = batch\n",
        "        user_emb, item_emb = self.base()\n",
        "        pos = self.base.score(u,i,user_emb,item_emb) + self.beta * self.geo_term(u,i,user_home,item_latlon)\n",
        "        neg = self.base.score(u,j,user_emb,item_emb) + self.beta * self.geo_term(u,j,user_home,item_latlon)\n",
        "        loss_bpr = bpr_loss(pos,neg,reg,[self.base.user_emb.weight, self.base.item_emb.weight])\n",
        "        cl_u = info_nce(user_emb, self.Uk)\n",
        "        cl_i = info_nce(item_emb, self.Vk)\n",
        "        loss = loss_bpr + self.lambda_cl*(cl_u + cl_i)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        return float(loss.item())\n",
        "\n",
        "def train_lightgcl_geo(epochs=5, emb_dim=64, batch_size=2048, lr=1e-3, reg=1e-4, lambda_cl=0.1, beta=0.2, R=5.0):\n",
        "    base = LightGCN(num_users, num_items, emb_dim=emb_dim).to(DEVICE)\n",
        "    model = LightGCL_Geo(base, beta=beta, R=R, lambda_cl=lambda_cl).to(DEVICE)\n",
        "    opt = torch.optim.Adam(base.parameters(), lr=lr)\n",
        "    sampler = bpr_triplet_sampler(train_df, num_items, radius_km=R,\n",
        "                                  item_latlon=item_latlon, user_pos_map=None,\n",
        "                                  user_home_latlon=user_home, batch_size=batch_size)\n",
        "    for ep in range(1, epochs+1):\n",
        "        total=0.0\n",
        "        for step in range(max(1, len(train_df)//batch_size)):\n",
        "            u,i,j = next(sampler)\n",
        "            total += model.training_step((u,i,j), opt, reg=reg)\n",
        "        print(f\"[LightGCL+Geo] epoch {ep} loss {total/(step+1):.4f}\")\n",
        "    return model\n",
        "\n",
        "lightgcl_geo_model = train_lightgcl_geo(epochs=5, emb_dim=64, beta=0.3, R=5.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94239d6d",
      "metadata": {
        "id": "94239d6d"
      },
      "source": [
        "Comparative Model 1: PinSage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28af8794",
      "metadata": {
        "id": "28af8794"
      },
      "outputs": [],
      "source": [
        "class PinSageItemEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, hidden_dim) if in_dim>0 else None\n",
        "        self.layers = nn.ModuleList([\n",
        "            PinSAGEConv(in_channels=hidden_dim, out_channels=hidden_dim, heads=1)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = x\n",
        "        if self.proj is not None:\n",
        "            h = self.proj(h)\n",
        "        for conv in self.layers:\n",
        "            h = conv(h, edge_index)\n",
        "            h = F.relu(h)\n",
        "        return F.normalize(h, dim=1)\n",
        "\n",
        "class PinSageRecommender(nn.Module):\n",
        "    def __init__(self, item_in_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.item_enc = PinSAGEItemEncoder(item_in_dim, hidden_dim=hidden_dim)\n",
        "\n",
        "    def forward(self, item_x, ii_edge):\n",
        "        return self.item_enc(item_x, ii_edge)\n",
        "\n",
        "    def user_repr(self, user_pos_items, item_emb):\n",
        "        # mean pool over positives\n",
        "        return F.normalize(item_emb[user_pos_items].mean(0, keepdim=True), dim=1)\n",
        "\n",
        "def train_pinsage(epochs=5, hidden_dim=64, batch_size=2048, lr=1e-3):\n",
        "    item_x = data['item'].x\n",
        "    ii = data['item','similar','item'].edge_index\n",
        "    model = PinSageRecommender(item_x.size(1), hidden_dim=hidden_dim).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    user_pos_map = {u:set(g[\"i\"].values.tolist()) for u,g in train_df.groupby(\"u\")}\n",
        "    sampler = bpr_triplet_sampler(train_df, num_items, batch_size=batch_size)\n",
        "    for ep in range(1, epochs+1):\n",
        "        total = 0.0\n",
        "        for step in range(max(1, len(train_df)//batch_size)):\n",
        "            u, i, j = next(sampler)\n",
        "            item_emb = model(item_x, ii)\n",
        "            u_repr = item_emb[i]  # treat pos item as anchor (approximate)\n",
        "            pos = (u_repr * item_emb[i]).sum(1)\n",
        "            neg = (u_repr * item_emb[j]).sum(1)\n",
        "            loss = -F.logsigmoid(pos - neg).mean()\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total += float(loss)\n",
        "        print(f\"[PinSage] epoch {ep} loss {total/(step+1):.4f}\")\n",
        "    return model\n",
        "\n",
        "pinsage_model = train_pinsage(epochs=5, hidden_dim=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd05290f",
      "metadata": {
        "id": "fd05290f"
      },
      "source": [
        "Comparative Model 2: LightGCN + HGT\n",
        "\n",
        "Elements:\n",
        "1. Base Model LightGCN\n",
        "2. HGT item representation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d2e9d0f",
      "metadata": {
        "id": "2d2e9d0f"
      },
      "outputs": [],
      "source": [
        "# Build minimal hetero metadata graph for items: (item)-[has_cat]->(cat), (item)-[has_price]->(price_level)\n",
        "# From df categories/price\n",
        "cat_encoder = LabelEncoder()\n",
        "all_cats = []\n",
        "for row in df[\"categories\"]:\n",
        "    if isinstance(row, list):\n",
        "        all_cats += row\n",
        "    elif pd.notna(row):\n",
        "        all_cats.append(str(row))\n",
        "if len(all_cats)==0:\n",
        "    all_cats = [\"unknown\"]\n",
        "cat_encoder.fit(list(set(all_cats)))\n",
        "num_cats = len(cat_encoder.classes_)\n",
        "price_levels = sorted(list(set([int(p) if pd.notna(p) else 0 for p in df[\"price\"]])))\n",
        "price_to_idx = {p:i for i,p in enumerate(price_levels)}\n",
        "num_prices = len(price_levels)\n",
        "\n",
        "meta = HeteroData()\n",
        "meta[\"item\"].num_nodes = num_items\n",
        "meta[\"cat\"].num_nodes = num_cats\n",
        "meta[\"price\"].num_nodes = num_prices\n",
        "# item->cat edges\n",
        "src, dst = [], []\n",
        "for i, g in df.groupby(\"i\"):\n",
        "    cats = g[\"categories\"].iloc[0]\n",
        "    if isinstance(cats, list) and len(cats)>0:\n",
        "        for c in cats[:3]:\n",
        "            src.append(i); dst.append(cat_encoder.transform([c])[0])\n",
        "    else:\n",
        "        src.append(i); dst.append(cat_encoder.transform([cat_encoder.classes_[0]])[0])\n",
        "meta[\"item\",\"has_cat\",\"cat\"].edge_index = torch.tensor([src,dst], dtype=torch.long)\n",
        "# item->price edges\n",
        "src, dst = [], []\n",
        "for i, g in df.groupby(\"i\"):\n",
        "    p = g[\"price\"].iloc[0]\n",
        "    p = int(p) if pd.notna(p) else 0\n",
        "    src.append(i); dst.append(price_to_idx.get(p, 0))\n",
        "meta[\"item\",\"has_price\",\"price\"].edge_index = torch.tensor([src,dst], dtype=torch.long)\n",
        "meta = meta.to(DEVICE)\n",
        "\n",
        "class HGTItemEncoder(nn.Module):\n",
        "    def __init__(self, hidden=64, heads=2, layers=2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.ModuleDict({\n",
        "            'item': nn.Embedding(num_items, hidden),\n",
        "            'cat': nn.Embedding(num_cats, hidden),\n",
        "            'price': nn.Embedding(num_prices, hidden),\n",
        "        })\n",
        "        for k in self.emb:\n",
        "            nn.init.xavier_uniform_(self.emb[k].weight)\n",
        "        self.layers = nn.ModuleList([\n",
        "            HGTConv(hidden_channels=hidden, out_channels=hidden, num_types=3, num_relations=2, heads=heads)\n",
        "            for _ in range(layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, meta: HeteroData):\n",
        "        x_dict = {k: self.emb[k].weight for k in ['item','cat','price']}\n",
        "        for conv in self.layers:\n",
        "            x_dict = conv(x_dict, meta.edge_index_dict)\n",
        "            x_dict = {k: F.relu(v) for k,v in x_dict.items()}\n",
        "        return x_dict['item']\n",
        "\n",
        "class LightGCN_HGT(nn.Module):\n",
        "    def __init__(self, lightgcn: LightGCN, hgt_hidden=64):\n",
        "        super().__init__()\n",
        "        self.lgcn = lightgcn\n",
        "        self.hgt = HGTItemEncoder(hidden=hgt_hidden)\n",
        "        self.fuse = nn.Linear(self.lgcn.user_emb.embedding_dim + hgt_hidden, self.lgcn.user_emb.embedding_dim)\n",
        "\n",
        "    def fused_item_emb(self):\n",
        "        user_e, item_e = self.lgcn()\n",
        "        hgt_item = self.hgt(meta)\n",
        "        item_fused = self.fuse(torch.cat([item_e, hgt_item], dim=1))\n",
        "        return user_e, item_fused\n",
        "\n",
        "    def score(self, users, items):\n",
        "        ue, ie = self.fused_item_emb()\n",
        "        return (ue[users]*ie[items]).sum(1)\n",
        "\n",
        "def train_lightgcn_hgt(epochs=5, emb_dim=64, batch_size=2048, lr=1e-3, reg=1e-4):\n",
        "    base = LightGCN(num_users, num_items, emb_dim=emb_dim).to(DEVICE)\n",
        "    model = LightGCN_HGT(base, hgt_hidden=emb_dim).to(DEVICE)\n",
        "    params = list(model.lgcn.parameters()) + list(model.hgt.parameters()) + list(model.fuse.parameters())\n",
        "    opt = torch.optim.Adam(params, lr=lr)\n",
        "    sampler = bpr_triplet_sampler(train_df, num_items, batch_size=batch_size)\n",
        "    for ep in range(1, epochs+1):\n",
        "        total=0.0\n",
        "        for step in range(max(1, len(train_df)//batch_size)):\n",
        "            u,i,j = next(sampler)\n",
        "            ue, ie = model.lgcn()\n",
        "            hgt_item = model.hgt(meta)\n",
        "            fused = model.fuse(torch.cat([ie, hgt_item], dim=1))\n",
        "            pos = (ue[u]*fused[i]).sum(1)\n",
        "            neg = (ue[u]*fused[j]).sum(1)\n",
        "            loss = bpr_loss(pos,neg,reg, params)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total += float(loss)\n",
        "        print(f\"[LightGCN+HGT] epoch {ep} loss {total/(step+1):.4f}\")\n",
        "    return model\n",
        "\n",
        "lghgt_model = train_lightgcn_hgt(epochs=5, emb_dim=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6266ac98",
      "metadata": {
        "id": "6266ac98"
      },
      "source": [
        "Evaluation Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "300782ec",
      "metadata": {
        "id": "300782ec"
      },
      "outputs": [],
      "source": [
        "def full_ranking_scores(model, model_name, K_list=(5,10), geo_R=5.0, geo_beta=0.0):\n",
        "    # prepare user->train positives and heldouts\n",
        "    train_pos = {u:set(g[\"i\"].values.tolist()) for u,g in train_df.groupby(\"u\")}\n",
        "    gt_val = {u:set(g[\"i\"].values.tolist()) for u,g in val_df.groupby(\"u\")}\n",
        "    gt_test = {u:set(g[\"i\"].values.tolist()) for u,g in test_df.groupby(\"u\")}\n",
        "    # compute item/user embeddings or scorers per model\n",
        "    with torch.no_grad():\n",
        "        if isinstance(model, LightGCN):\n",
        "            ue, ie = model()\n",
        "            def score_u(u):\n",
        "                # mask train positives for ranking\n",
        "                s = (ue[u].unsqueeze(0) * ie).sum(1)\n",
        "                for it in train_pos.get(u,[]):\n",
        "                    s[it] = -1e9\n",
        "                return s\n",
        "        elif isinstance(model, LightGCL) and not isinstance(model, LightGCL_Geo):\n",
        "            ue, ie = model.base()\n",
        "            def score_u(u):\n",
        "                s = (ue[u].unsqueeze(0) * ie).sum(1)\n",
        "                for it in train_pos.get(u,[]): s[it] = -1e9\n",
        "                return s\n",
        "        elif isinstance(model, LightGCL_Geo):\n",
        "            ue, ie = model.base()\n",
        "            def score_u(u):\n",
        "                s = (ue[u].unsqueeze(0) * ie).sum(1)\n",
        "                # add geo distance term\n",
        "                geo = []\n",
        "                for it in range(num_items):\n",
        "                    uh = user_home.get(u)\n",
        "                    lati,loni = item_latlon[it]\n",
        "                    w = 0.0\n",
        "                    if uh and not any(np.isnan(uh)) and not math.isnan(lati) and not math.isnan(loni):\n",
        "                        d = haversine(uh,(lati,loni))\n",
        "                        w = math.exp(-d/model.R)\n",
        "                    geo.append(w)\n",
        "                s = s + model.beta*torch.tensor(geo, device=DEVICE)\n",
        "                for it in train_pos.get(u,[]): s[it] = -1e9\n",
        "                return s\n",
        "        elif isinstance(model, PinSageRecommender):\n",
        "            item_x = data['item'].x\n",
        "            ii = data['item','similar','item'].edge_index\n",
        "            item_emb = model(item_x, ii)\n",
        "            def score_u(u):\n",
        "                pos = list(train_pos.get(u, [])) or [0]\n",
        "                uvec = item_emb[pos].mean(0, keepdim=True)\n",
        "                s = (uvec * item_emb).sum(1)\n",
        "                for it in train_pos.get(u,[]): s[it] = -1e9\n",
        "                return s\n",
        "        elif isinstance(model, LightGCN_HGT):\n",
        "            def score_u(u):\n",
        "                ue, ie = model.fused_item_emb()\n",
        "                s = (ue[u].unsqueeze(0) * ie).sum(1)\n",
        "                for it in train_pos.get(u,[]): s[it] = -1e9\n",
        "                return s\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model for evaluation\")\n",
        "\n",
        "        def evaluate(gt_dict, tag):\n",
        "            rec, ndcg, gndcg, rr = [], [], [], []\n",
        "            for u in gt_dict.keys():\n",
        "                s = score_u(u)\n",
        "                ranked = torch.topk(s, k=min(100, num_items)).indices.tolist()\n",
        "                truth = gt_dict[u]\n",
        "                rec.append(recall_at_k(ranked, truth, k=max(K_list)))\n",
        "                ndcg.append(ndcg_at_k(ranked, truth, k=max(K_list)))\n",
        "                # geo-aware NDCG\n",
        "                u_loc = user_home.get(u)\n",
        "                gndcg.append(geo_ndcg_at_k(ranked, truth, u_loc, item_latlon, k=max(K_list), R=geo_R))\n",
        "                rr.append(mrr(ranked, truth, k=max(K_list)))\n",
        "            return np.mean(rec), np.mean(ndcg), np.mean(gndcg), np.mean(rr)\n",
        "\n",
        "        val_scores = evaluate(gt_val, \"val\")\n",
        "        test_scores = evaluate(gt_test, \"test\")\n",
        "        print(f\"[{model_name}] Val  Recall@K:{val_scores[0]:.4f}  NDCG@K:{val_scores[1]:.4f}  GeoNDCG@K:{val_scores[2]:.4f}  MRR:{val_scores[3]:.4f}\")\n",
        "        print(f\"[{model_name}] Test Recall@K:{test_scores[0]:.4f}  NDCG@K:{test_scores[1]:.4f}  GeoNDCG@K:{test_scores[2]:.4f}  MRR:{test_scores[3]:.4f}\")\n",
        "        return {\"model\": model_name, \"val_recall\":val_scores[0], \"val_ndcg\":val_scores[1], \"val_geondcg\":val_scores[2], \"val_mrr\":val_scores[3],\n",
        "                \"test_recall\":test_scores[0], \"test_ndcg\":test_scores[1], \"test_geondcg\":test_scores[2], \"test_mrr\":test_scores[3]}\n",
        "\n",
        "results = []\n",
        "results.append(full_ranking_scores(lightgcn_model, \"LightGCN\"))\n",
        "results.append(full_ranking_scores(lightgcl_model.base, \"LightGCL (base view)\"))\n",
        "results.append(full_ranking_scores(lightgcl_geo_model.base, \"LightGCL+Geo (inference uses geo)\", geo_R=5.0))\n",
        "results.append(full_ranking_scores(pinsage_model, \"PinSage\"))\n",
        "results.append(full_ranking_scores(lghgt_model.lgcn, \"LightGCN+HGT (fused)\"))\n",
        "pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cecca63",
      "metadata": {
        "id": "8cecca63"
      },
      "source": [
        "Rating Metrics (RMSE/MAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8838d9a2",
      "metadata": {
        "id": "8838d9a2"
      },
      "outputs": [],
      "source": [
        "def pointwise_eval(model, which=\"val\"):\n",
        "    df_eval = val_df if which==\"val\" else test_df\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        if isinstance(model, LightGCN):\n",
        "            ue, ie = model()\n",
        "            for _, r in df_eval.iterrows():\n",
        "                preds.append(float((ue[r.u] * ie[r.i]).sum().item()))\n",
        "                trues.append(float(r.rating))\n",
        "        elif isinstance(model, LightGCL):\n",
        "            ue, ie = model.base()\n",
        "            for _, r in df_eval.iterrows():\n",
        "                s = float((ue[r.u]*ie[r.i]).sum().item())\n",
        "                preds.append(s); trues.append(float(r.rating))\n",
        "        elif isinstance(model, LightGCL_Geo):\n",
        "            ue, ie = model.base()\n",
        "            for _, r in df_eval.iterrows():\n",
        "                s = float((ue[r.u]*ie[r.i]).sum().item())\n",
        "                # add geo term\n",
        "                uh = user_home.get(int(r.u))\n",
        "                lati,loni = item_latlon[int(r.i)]\n",
        "                if uh and not any(np.isnan(uh)) and not math.isnan(lati) and not math.isnan(loni):\n",
        "                    d = haversine(uh,(lati,loni))\n",
        "                    s += model.beta*math.exp(-d/model.R)\n",
        "                preds.append(s); trues.append(float(r.rating))\n",
        "        elif isinstance(model, PinSageRecommender):\n",
        "            item_x = data['item'].x\n",
        "            ii = data['item','similar','item'].edge_index\n",
        "            item_emb = model(item_x, ii)\n",
        "            for _, r in df_eval.iterrows():\n",
        "                # approximate user vector by mean of their train items\n",
        "                pos = train_df[train_df.u==r.u][\"i\"].values\n",
        "                if len(pos)==0: continue\n",
        "                uvec = item_emb[pos].mean(0)\n",
        "                preds.append(float((uvec * item_emb[r.i]).sum().item()))\n",
        "                trues.append(float(r.rating))\n",
        "        elif isinstance(model, LightGCN_HGT):\n",
        "            ue, ie = model.fused_item_emb()\n",
        "            for _, r in df_eval.iterrows():\n",
        "                preds.append(float((ue[r.u]*ie[r.i]).sum().item()))\n",
        "                trues.append(float(r.rating))\n",
        "        else:\n",
        "            raise ValueError(\"unknown model\")\n",
        "    return {\"rmse\": rmse(preds,trues), \"mae\": mae(preds,trues), \"n\": len(trues)}\n",
        "\n",
        "for name, mdl in [\n",
        "    (\"LightGCN\", lightgcn_model),\n",
        "    (\"LightGCL\", lightgcl_model),\n",
        "    (\"LightGCL+Geo\", lightgcl_geo_model),\n",
        "    (\"PinSage\", pinsage_model),\n",
        "    (\"LightGCN+HGT\", lghgt_model),\n",
        "]:\n",
        "    pe = pointwise_eval(mdl, \"test\")\n",
        "    print(f\"{name} rating RMSE={pe['rmse']:.4f} MAE={pe['mae']:.4f} (n={pe['n']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1b4ae7",
      "metadata": {
        "id": "3d1b4ae7"
      },
      "source": [
        "Vistualization: Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bc962c",
      "metadata": {
        "id": "b8bc962c"
      },
      "outputs": [],
      "source": [
        "summary = pd.DataFrame(results).assign(\n",
        "    rmse=[pointwise_eval(m, 'test')[\"rmse\"] for m in [lightgcn_model, lightgcl_model, lightgcl_geo_model, pinsage_model, lghgt_model]],\n",
        "    mae =[pointwise_eval(m, 'test')[\"mae\"]  for m in [lightgcn_model, lightgcl_model, lightgcl_geo_model, pinsage_model, lghgt_model]],\n",
        ")\n",
        "summary = summary[[\"model\",\"test_recall\",\"test_ndcg\",\"test_geondcg\",\"test_mrr\",\"rmse\",\"mae\"]]\n",
        "summary.sort_values(\"test_ndcg\", ascending=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}